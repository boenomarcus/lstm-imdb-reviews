{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "FURB - UNIVERSIDADE REGIONAL DE BLUMENAU\n",
    "\n",
    "Especialização em Data Science - Turma 2\n",
    "\n",
    "Disciplina: Deep Learning\n",
    "\n",
    "**Nome: Marcus Moresco Boeno**\n",
    "\n",
    "O presente estudo foi desenvolvido como atividade final da disciplina de Deep Learning da Pós-Graduação em Data Science da FURB (Universidade Regional de Blumenau).\n",
    "\n",
    "A atividade consiste no desenvolvimento e teste de uma arquitetura de rede neural recorrente para a classificação de sentimento em críticas de filmes utilizando o framework [PyTorch](https://pytorch.org/).\n",
    "\n",
    "O dataset utilizado é composto por 50 mil revisões de filmes na plataforma [IMDB](https://ai.stanford.edu/~amaas/data/sentiment/), sendo disponibilizado por meio do próprio PyTorch pela classe `torchtext.datasets.IMDB()`.\n",
    "\n",
    "A rede neural construída se trata de uma arquitetura do tipo recorrente com células LSTM seguidas de uma camada de dropout e uma camada totalmente conectada para predição da classe (sentimento) da revisão. A rede é alimentada com sequencias de tokens mapeados para vetores densos de 30 dimensões por um modelo Word2Vec treinado com a própria base de dados. \n",
    "\n",
    "A rede está estruturada da seguinte maneira:\n",
    "\n",
    "- Camada de Embedding com vetores treinados com modelo Word2Vec;\n",
    "- Camada Recorrente LSTM com inicialização Xavier;\n",
    "- Camada Dropout com manutenção de 50% dos neurônios;\n",
    "- Camada totalmente conectada com ativação sigmoidal.\n",
    "\n",
    "A seguir são apresentados os passos de pré-processamento, desenvolvimento da rede e otimização de hiperparâmetros, sendo que o presente notebook está estruturado da seguinte forma:\n",
    "\n",
    "Estrutura do Notebook\n",
    "- Importando bibliotecas\n",
    "- Leitura da Base de Dados\n",
    "- Pré-Processamento\n",
    "    - Tokenização, lematização e remoção de tags HTML, pontuação e stopwords\n",
    "    - Detecção e mapeamento de palavras compostas\n",
    "    - Treinamento de modelo Word2Vec\n",
    "    - Definição da matriz de vetores densos\n",
    "    - Mapeamento de tokens para índices e aplicação de padding\n",
    "- Definição dos conjuntos de treino, validação e teste\n",
    "- Implementação de Rede Neural LSTM\n",
    "- Treinamento e otimização de hiperparâmetros\n",
    "- Análise de acurácia\n",
    "- Carregando o modelo treinado e realizando classificações\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Importando bibliotecas\n",
    "---\n",
    "\n",
    "Em um primeiro momento foram carregadas bibliotecas, classes e métodos para manipulação dos dados e implementação da rede neural.\n",
    "\n",
    "A rede será implementada com o auxílio do framework [PyTorch](https://pytorch.org/). \n",
    "\n",
    "As bibliotecas `NumPy`, `Pandas`, `random` e `pickle` serão utilizadas para leitura/export do dataset e manipulação de dataframes e arrays.\n",
    "\n",
    "As bibliotecas `random`, `re`, `nltk` e `BeatifulSoup` serão utilizadas nas etapas de pré-processamento das reviews.\n",
    "\n",
    "A biblioteca `gensim` será utilizada para o treinamento de um modelo Word2Vec e a composição do vocabulário a ser inserido no modelo.\n",
    "\n",
    "A biblioteca `Scikit-learn` será utilizada para auxílido nas etapas de obteção de conjuntos de treinamento e validação (método `train_test_split`).\n",
    "\n",
    "A biblioteca `optuna` será utilizada para a etapa de otimização dos hiperparâmetros da rede LSTM.\n",
    "\n",
    "Por fim, a biblioteca `pickle` servirá como base para o registro dos resultados do experimento como arquivos binários com a extensão \".pkl\". Dessa forma, etapas do experimento podem carregadas facilmente em rotinas futuras.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "\n",
    "# Import para download da base de dados IMDB\n",
    "from torchtext import datasets\n",
    "\n",
    "# Imports para pré-processamento das strings\n",
    "import random\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Imports para captura de palavras compostas e word embedding\n",
    "from gensim.models import Word2Vec, Phrases\n",
    "\n",
    "# Manipulação de strings, arrays e dataframes\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "# Import do PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Import do Optuna para otimimização de hiperparâmetros\n",
    "import optuna\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/marcus/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/marcus/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-10T00:44:12.514627Z",
     "start_time": "2021-06-10T00:44:12.509125Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Leitura da Base de Dados\n",
    "---\n",
    "\n",
    "Após realizar o download e descompactar o dataset com o método torchtext.datasets.IMDB(), carregamos os dados diretamente dos arquivos txt em um pandas Dataframe. Em um primeiro momento não utilizaremos iteradores para termos maior flexibilidade no desenvolvimento e aplicação dos métodos de pré-processamento.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "\n",
    "def load_data() -> pd.DataFrame:\n",
    "    \"\"\"Leitura de Reviews do IMDB\n",
    "\n",
    "    > Argumentos:\n",
    "        - Sem argumentos\n",
    "    \n",
    "    > Outputs:\n",
    "        - (pd.DataFrame): Dataframe com reviews presentes no dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    # Recupera lista de arquivos\n",
    "    files = glob('.data/IMDB/aclImdb/train/neg/*.txt')\n",
    "    files.extend(glob('.data/IMDB/aclImdb/train/pos/*.txt'))\n",
    "    files.extend(glob('.data/IMDB/aclImdb/test/neg/*.txt'))\n",
    "    files.extend(glob('.data/IMDB/aclImdb/test/pos/*.txt'))\n",
    "    random.shuffle(files)\n",
    "    \n",
    "    # Carrega dados em forma de dicionário\n",
    "    reviews_dict = {k:[] for k in ['filename', 'usage', 'sent', 'text']}\n",
    "    for f in files:\n",
    "        _, _, _, use, sent, fname = f.split('/')\n",
    "        reviews_dict['filename'].append(fname)\n",
    "        reviews_dict['usage'].append(use)\n",
    "        reviews_dict['sent'].append(sent)\n",
    "        with open(f, 'r') as g:\n",
    "            phrase = g.read()\n",
    "        reviews_dict['text'].append(phrase)\n",
    "    reviews = pd.DataFrame(reviews_dict)\n",
    "    reviews = reviews.sample(frac=1).reset_index(drop=True)    \n",
    "\n",
    "    # Retorna dados\n",
    "    return reviews\n",
    "\n",
    "# Carrega dataset de reviews do IMDB\n",
    "imdb_reviews = load_data()\n",
    "\n",
    "# Apresenta head e tail do dataframe\n",
    "imdb_reviews\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          filename  usage sent  \\\n",
       "0        494_9.txt  train  pos   \n",
       "1      11293_3.txt  train  neg   \n",
       "2       6429_1.txt   test  neg   \n",
       "3       9913_8.txt   test  pos   \n",
       "4       4421_4.txt  train  neg   \n",
       "...            ...    ...  ...   \n",
       "49995  4384_10.txt  train  pos   \n",
       "49996   5438_3.txt  train  neg   \n",
       "49997   7743_7.txt   test  pos   \n",
       "49998  3084_10.txt   test  pos   \n",
       "49999   3259_7.txt   test  pos   \n",
       "\n",
       "                                                    text  \n",
       "0      This is the start of a new and interesting Sta...  \n",
       "1      This movie was a real torture fest to sit thro...  \n",
       "2      What can be said about such a pathetic movie ?...  \n",
       "3      Kirk and the crew are visiting a federation mi...  \n",
       "4      The subject is certainly compelling: a group o...  \n",
       "...                                                  ...  \n",
       "49995  MAJOR SPOILERS!! THIS IS FOR PEOPLE WHO HAVE S...  \n",
       "49996  I am quite a fan of novelist/screenwriter Mich...  \n",
       "49997  For many years I thought I was the only person...  \n",
       "49998  Cary Elwes have to say puts on a better perfor...  \n",
       "49999  I had never heard of this movie, but I like He...  \n",
       "\n",
       "[50000 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>usage</th>\n",
       "      <th>sent</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>494_9.txt</td>\n",
       "      <td>train</td>\n",
       "      <td>pos</td>\n",
       "      <td>This is the start of a new and interesting Sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11293_3.txt</td>\n",
       "      <td>train</td>\n",
       "      <td>neg</td>\n",
       "      <td>This movie was a real torture fest to sit thro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6429_1.txt</td>\n",
       "      <td>test</td>\n",
       "      <td>neg</td>\n",
       "      <td>What can be said about such a pathetic movie ?...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9913_8.txt</td>\n",
       "      <td>test</td>\n",
       "      <td>pos</td>\n",
       "      <td>Kirk and the crew are visiting a federation mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4421_4.txt</td>\n",
       "      <td>train</td>\n",
       "      <td>neg</td>\n",
       "      <td>The subject is certainly compelling: a group o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>4384_10.txt</td>\n",
       "      <td>train</td>\n",
       "      <td>pos</td>\n",
       "      <td>MAJOR SPOILERS!! THIS IS FOR PEOPLE WHO HAVE S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>5438_3.txt</td>\n",
       "      <td>train</td>\n",
       "      <td>neg</td>\n",
       "      <td>I am quite a fan of novelist/screenwriter Mich...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>7743_7.txt</td>\n",
       "      <td>test</td>\n",
       "      <td>pos</td>\n",
       "      <td>For many years I thought I was the only person...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>3084_10.txt</td>\n",
       "      <td>test</td>\n",
       "      <td>pos</td>\n",
       "      <td>Cary Elwes have to say puts on a better perfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>3259_7.txt</td>\n",
       "      <td>test</td>\n",
       "      <td>pos</td>\n",
       "      <td>I had never heard of this movie, but I like He...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 4 columns</p>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "O dataset é composto de 50000 revisões de filmes realizadas no portal IMDB, sendo 25000 classificadas como \"positiva\" e 25000 como \"negativa\". Cada grupo ainda é subdivido em 12500 amostras para treino e 12500 para teste. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Pré-processamento\n",
    "---\n",
    "\n",
    "O pré-processamento é uma etapa muito importante no desenvolvimento de um modelo de classificação de texto. Aqui vamos retirar do dataset caracteres irrelevantes como tags HTML, pontuações e palavras que possuem pouco valor semântico (stopwords). Dessa forma realizamos uma \"limpeza\" no dataset, entregando ao modelo apenas palavras que tenham mais imporância para a análise de sentimento.\n",
    "\n",
    "Também realizaremos a detecção e mapeamento de palavras compostas dentro do dataset, visando incrementar o valor semântico do vocabulário usado para o treinamento da rede neural.\n",
    "\n",
    "Após as etapas de pré-processamento iremos realizar o treinamento de um modelo Word2Vec para transformarmos palavras em vetores densos n-dimensionais, para que a rede neural possa interpretar as frases que compõem cada revisão.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Tokenização, lematização e remoção de tags HTML, pontuação e stopwords\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\n",
    "# Compila regex para caracteres\n",
    "CHAR_REGEX = re.compile(r'[^A-Za-z\\s]')\n",
    "\n",
    "def sentence_cleaning(txt:str):\n",
    "    \"\"\"Realiza limpeza das frases\"\"\"\n",
    "\n",
    "    # Remove tags HTML\n",
    "    txt = BeautifulSoup(txt, \"html.parser\")\n",
    "    txt = txt.get_text()\n",
    "\n",
    "    # Recupera apenas letras\n",
    "    txt = CHAR_REGEX.sub(\" \", txt)\n",
    "\n",
    "    # Passa todos os caracteres para letras minúsculas\n",
    "    txt = txt.lower().strip()\n",
    "\n",
    "    # Cria tokens\n",
    "    txt =  word_tokenize(txt, \"english\")\n",
    "\n",
    "    # Realiza lemmatize para cada palavra\n",
    "    lem = WordNetLemmatizer()\n",
    "    txt = [lem.lemmatize(w, pos='v') for w in txt]\n",
    "\n",
    "    # Remove stopwords\n",
    "    txt = [w for w in txt if w not in stopwords.words('english')]\n",
    "    \n",
    "    # Retorna texto\n",
    "    return \" \".join(txt)\n",
    "\n",
    "\n",
    "# Realiza primeira etapa do pré-processamento das reviews\n",
    "imdb_reviews['text_clean'] = imdb_reviews['text'].apply(sentence_cleaning)\n",
    "\n",
    "# Apresenta primeiras linhas\n",
    "imdb_reviews.head()\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      filename  usage sent                                               text  \\\n",
       "0    494_9.txt  train  pos  This is the start of a new and interesting Sta...   \n",
       "1  11293_3.txt  train  neg  This movie was a real torture fest to sit thro...   \n",
       "2   6429_1.txt   test  neg  What can be said about such a pathetic movie ?...   \n",
       "3   9913_8.txt   test  pos  Kirk and the crew are visiting a federation mi...   \n",
       "4   4421_4.txt  train  neg  The subject is certainly compelling: a group o...   \n",
       "\n",
       "                                          text_clean  \n",
       "0  start new interest star trek series earth kind...  \n",
       "1  movie real torture fest sit first mistake trea...  \n",
       "2  say pathetic movie bad act main actress seem k...  \n",
       "3  kirk crew visit federation mine colony remote ...  \n",
       "4  subject certainly compel group people take lov...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>usage</th>\n",
       "      <th>sent</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>494_9.txt</td>\n",
       "      <td>train</td>\n",
       "      <td>pos</td>\n",
       "      <td>This is the start of a new and interesting Sta...</td>\n",
       "      <td>start new interest star trek series earth kind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11293_3.txt</td>\n",
       "      <td>train</td>\n",
       "      <td>neg</td>\n",
       "      <td>This movie was a real torture fest to sit thro...</td>\n",
       "      <td>movie real torture fest sit first mistake trea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6429_1.txt</td>\n",
       "      <td>test</td>\n",
       "      <td>neg</td>\n",
       "      <td>What can be said about such a pathetic movie ?...</td>\n",
       "      <td>say pathetic movie bad act main actress seem k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9913_8.txt</td>\n",
       "      <td>test</td>\n",
       "      <td>pos</td>\n",
       "      <td>Kirk and the crew are visiting a federation mi...</td>\n",
       "      <td>kirk crew visit federation mine colony remote ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4421_4.txt</td>\n",
       "      <td>train</td>\n",
       "      <td>neg</td>\n",
       "      <td>The subject is certainly compelling: a group o...</td>\n",
       "      <td>subject certainly compel group people take lov...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "\n",
    "# Exemplo do output da primeira etapa de pré-processamento \n",
    "print(\"Exemplo do output da primeira etapa de pré-processamento\\n\")\n",
    "print(\"Frase Original:\")\n",
    "print(imdb_reviews.iloc[0, 3])\n",
    "print(\"\\nFrase após primeira etapa de pré-processamento:\")\n",
    "print(imdb_reviews.iloc[0, 4])\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Exemplo do output da primeira etapa de pré-processamento\n",
      "\n",
      "Frase Original:\n",
      "This is the start of a new and interesting Star Trek series. It has a \"down to earth\"-kind of feel with darker and less \"plaggy\" scenography.<br /><br />The characters need some more time to develop but they have potential. One thing that is fairly disappointing (with all Star Trek series really) is that they portray such a gloomy picture of the equality between men and women in the future when they paint a very positive picture about everything else. (Earth has stopped war, famine etc)<br /><br /> The female characters here are two, subcommander T'Pol who is vulcan and communications officer Hoshi who is human. Hoshi is quite wimpy and T'Pol is made to be a \"vulcan babe\".<br /><br /> Some of the crew attitudes feel a bit too American (as opposed to the more international feel of the TNG-crew) but creates interesting dynamics.<br /><br /> A very good pilot though for a very good series.\n",
      "\n",
      "Frase após primeira etapa de pré-processamento:\n",
      "start new interest star trek series earth kind feel darker less plaggy scenography character need time develop potential one thing fairly disappoint star trek series really portray gloomy picture equality men women future paint positive picture everything else earth stop war famine etc female character two subcommander pol vulcan communications officer hoshi human hoshi quite wimpy pol make vulcan babe crew attitudes feel bite american oppose international feel tng crew create interest dynamics good pilot though good series\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Detecção e mapeamento de palavras compostas\n",
    "---\n",
    "\n",
    "Agora, vamos utilizar a classe Phrases do Gensim, para capturar palavras compostas com comprimento de até três palavras. Com isso, os tokens \"new\" e \"york\", serão unidos como \"new_york\", assim como os tokes \"new\", \"york\" e \"city\" serão unidos como \"new_york_city\". Isso ajudará o nosso modelo a entender o contexto e a semântica de palavras compostas. Entretanto é válido salientar que o Gensim só irá reconhecer palavras compostas se eles tiverem grande frequência dentro da base de dados. Por exemplo, se as palavras \"star\" e \"wars\" aparecerem em sequencia frequentemente nas frases, o Gensim vai enteder que os tokens \"star\" e \"wars\" devem ser unidos em um token composto (\"star_wars\") quando aparecerem juntos. Dai a importância de uma base de dados extensa para o treinamento dos modelos."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "\n",
    "# Cria modelo para bigrams\n",
    "bigrams = Phrases(\n",
    "    sentences=imdb_reviews['text_clean'].apply(\n",
    "        lambda x: x.split()\n",
    "    ).values\n",
    ")\n",
    "\n",
    "# Cria modelo para trigrams \n",
    "trigrams = Phrases(\n",
    "    sentences=bigrams[imdb_reviews['text_clean'].apply(\n",
    "        lambda x: x.split()\n",
    "    ).values]\n",
    ")\n",
    "\n",
    "# Exportando bigrams para eventual leitura\n",
    "with open(\"pkl/bigrams.pkl\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(bigrams))\n",
    "\n",
    "# Exportando trigrams para eventual leitura\n",
    "with open(\"pkl/trigrams.pkl\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(trigrams))\n",
    "\n",
    "# Exemplo união de palavras compostas\n",
    "frase1 = 'the new york city skyline is amazing'\n",
    "frase2 = 'ewan mcgregor plays obiwan kenoni in star wars'\n",
    "\n",
    "print(f\"\\nOriginal: {frase1}\")\n",
    "print(f\"Modelo bigrams: {bigrams[frase1.split()]}\")\n",
    "print(f\"Modelo trigrams: {trigrams[bigrams[frase1.split()]]}\")\n",
    "\n",
    "print(f\"\\nOriginal: {frase2}\")\n",
    "print(f\"Modelo bigrams: {bigrams[frase2.split()]}\")\n",
    "print(f\"Modelo trigrams: {trigrams[bigrams[frase2.split()]]}\\n\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Original: the new york city skyline is amazing\n",
      "Modelo bigrams: ['the', 'new_york', 'city', 'skyline', 'is', 'amazing']\n",
      "Modelo trigrams: ['the', 'new_york_city', 'skyline', 'is', 'amazing']\n",
      "\n",
      "Original: ewan mcgregor plays obiwan kenoni in star wars\n",
      "Modelo bigrams: ['ewan_mcgregor', 'plays', 'obiwan', 'kenoni', 'in', 'star', 'wars']\n",
      "Modelo trigrams: ['ewan_mcgregor', 'plays', 'obiwan', 'kenoni', 'in', 'star', 'wars']\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "\n",
    "# Realiza mapeamento de palavras compostas no dataset\n",
    "imdb_reviews['ngrams'] = trigrams[bigrams[imdb_reviews['text_clean'].apply(\n",
    "    lambda x: x.split()\n",
    ").values]]\n",
    "\n",
    "# Apresenta primeiras linhas\n",
    "imdb_reviews.head()\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/home/marcus/miniconda3/envs/neural-nets/lib/python3.8/site-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      filename  usage sent                                               text  \\\n",
       "0    494_9.txt  train  pos  This is the start of a new and interesting Sta...   \n",
       "1  11293_3.txt  train  neg  This movie was a real torture fest to sit thro...   \n",
       "2   6429_1.txt   test  neg  What can be said about such a pathetic movie ?...   \n",
       "3   9913_8.txt   test  pos  Kirk and the crew are visiting a federation mi...   \n",
       "4   4421_4.txt  train  neg  The subject is certainly compelling: a group o...   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  start new interest star trek series earth kind...   \n",
       "1  movie real torture fest sit first mistake trea...   \n",
       "2  say pathetic movie bad act main actress seem k...   \n",
       "3  kirk crew visit federation mine colony remote ...   \n",
       "4  subject certainly compel group people take lov...   \n",
       "\n",
       "                                              ngrams  \n",
       "0  [start, new, interest, star_trek_series, earth...  \n",
       "1  [movie, real, torture, fest, sit, first, mista...  \n",
       "2  [say, pathetic, movie, bad, act, main_actress,...  \n",
       "3  [kirk_crew, visit, federation, mine, colony, r...  \n",
       "4  [subject, certainly, compel, group, people, ta...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>usage</th>\n",
       "      <th>sent</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>ngrams</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>494_9.txt</td>\n",
       "      <td>train</td>\n",
       "      <td>pos</td>\n",
       "      <td>This is the start of a new and interesting Sta...</td>\n",
       "      <td>start new interest star trek series earth kind...</td>\n",
       "      <td>[start, new, interest, star_trek_series, earth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11293_3.txt</td>\n",
       "      <td>train</td>\n",
       "      <td>neg</td>\n",
       "      <td>This movie was a real torture fest to sit thro...</td>\n",
       "      <td>movie real torture fest sit first mistake trea...</td>\n",
       "      <td>[movie, real, torture, fest, sit, first, mista...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6429_1.txt</td>\n",
       "      <td>test</td>\n",
       "      <td>neg</td>\n",
       "      <td>What can be said about such a pathetic movie ?...</td>\n",
       "      <td>say pathetic movie bad act main actress seem k...</td>\n",
       "      <td>[say, pathetic, movie, bad, act, main_actress,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9913_8.txt</td>\n",
       "      <td>test</td>\n",
       "      <td>pos</td>\n",
       "      <td>Kirk and the crew are visiting a federation mi...</td>\n",
       "      <td>kirk crew visit federation mine colony remote ...</td>\n",
       "      <td>[kirk_crew, visit, federation, mine, colony, r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4421_4.txt</td>\n",
       "      <td>train</td>\n",
       "      <td>neg</td>\n",
       "      <td>The subject is certainly compelling: a group o...</td>\n",
       "      <td>subject certainly compel group people take lov...</td>\n",
       "      <td>[subject, certainly, compel, group, people, ta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Treinamento de modelo Word2Vec\n",
    "---\n",
    "\n",
    "Após o pré-processamento das frases realizamos então o treinamento de um modelo Word2Vec para a criação do vocabulário que será utilizado para o treinamento da rede LSTM"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "\n",
    "# Indica início do treinamento\n",
    "print(\"Treinando modelo Word2Vec ...\", end=\"\")\n",
    "\n",
    "# Comprimento do vetor denso\n",
    "vec_dims = 30\n",
    "\n",
    "# Cria word embedding (word2vec) baseado nos exemplos disponíveis\n",
    "w2v_model = Word2Vec(\n",
    "    sentences = imdb_reviews['ngrams'].values,\n",
    "    vector_size = vec_dims,\n",
    "    min_count=3, \n",
    "    window=5, \n",
    "    workers=4\n",
    ")\n",
    "\n",
    "# Exportando modelo word2vec para eventual leitura\n",
    "with open(\"pkl/w2v_model.pkl\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(w2v_model))\n",
    "\n",
    "# Indica final do treinamento\n",
    "print(\" OK\")\n",
    "\n",
    "# Indica tamanho do vocabulário aprendido\n",
    "print(\"\\nQuantidade de palavras no vocabulário:\", len(w2v_model.wv))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Treinando modelo Word2Vec ... OK\n",
      "\n",
      "Quantidade de palavras no vocabulário: 64360\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "\n",
    "# Apresenta exemplo de vetor denso presente no modelo\n",
    "print(\"Vetor denso (30 dimensões) para a palavra 'edmund':\")\n",
    "print(w2v_model.wv.get_vector('edmund'), \"\\n\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Vetor denso (30 dimensões) para a palavra 'edmund':\n",
      "[-0.2779333  -0.6261392  -0.46311194  0.32009107  0.05335054 -0.2552865\n",
      "  1.0484984   0.11994497 -0.6145124  -0.2872367   0.78682846  0.65064883\n",
      "  0.2562991  -0.41484794 -0.6296202  -0.31579876  0.6088981  -0.3728812\n",
      " -0.5270643  -0.19861312 -0.64210504 -0.09350853  0.5085858   0.57208025\n",
      "  0.7300312   0.398961   -0.5370014   0.63785976  0.2984009  -1.0006626 ] \n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Definição da matriz de vetores densos\n",
    "---\n",
    "\n",
    "Neste passo iremos preparar a matriz de vetores densos para a camada de embedding que será inserida em nossa rede neural. A matriz terá dimensões iguais ao tamanho do vocabulário pela dimensão do vetor denso n-dimensional. Dessa forma, como nosso vocabulário é composto de 64360 palavras, teremos uma matriz com dimensões 64360 x 30. A matriz atuará como uma espécie de \"dicionário\". As sequências de tokens serão mapeadas para sequências de índices que remetem a posição do token na matriz de vetores densos. Para tratar o uso de padding no treinamento do modelo, adicionaremos um vetor preenchido com zeros. Dessa forma o padding será tratado como o índice 64360.\n",
    "  "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "\n",
    "# Matriz com pesos para camada de embedding\n",
    "print(f\"Dimensões matriz: {w2v_model.wv.vectors.shape}\")\n",
    "\n",
    "# Vetor de dimensões (1, 30) totalmente zerado será adicionado ao final\n",
    "# do vocabulário para atuar como um vetor de padding\n",
    "w2v_vectors = np.append(w2v_model.wv.vectors, np.zeros((1, vec_dims)), axis=0)\n",
    "print(f\"Dimensões nova matriz: {w2v_vectors.shape}\")\n",
    "\n",
    "# Transforma matriz em tensor para inserir na camada de embedding\n",
    "# da rede neural\n",
    "weights_tensor = torch.tensor(w2v_vectors.astype(np.float32))\n",
    "\n",
    "# Exportando matriz de vetores densos para eventual leitura\n",
    "with open(\"pkl/weights_tensor.pkl\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(weights_tensor))\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Dimensões matriz: (64360, 30)\n",
      "Dimensões nova matriz: (64361, 30)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Mapeamento de tokens para índices e aplicação de padding\n",
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "\n",
    "# Cria info do número de tokens\n",
    "imdb_reviews['n_tokens'] = imdb_reviews.ngrams.apply(lambda x: len(x))\n",
    "\n",
    "# Apresenta gráfico com densidade do número de tokens\n",
    "imdb_reviews['n_tokens'].describe()\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "count    50000.000000\n",
       "mean       105.734100\n",
       "std         80.906842\n",
       "min          3.000000\n",
       "25%         56.000000\n",
       "50%         78.000000\n",
       "75%        128.000000\n",
       "max       1329.000000\n",
       "Name: n_tokens, dtype: float64"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A quantidade máxima de tokens por frase será definida como 130 (valor um pouco acima do terceiro quartil).\n",
    "\n",
    "Tendo em mente a quantidade máxima de tokens por frase (130), definiu-se a função `map_index_and_pad()` para realizar o truncamento ou padding (adicionando à esquerda) em cada sequencia de tokens. Assim todas as frases do dataset terão um comprimento padrão de 130 índices."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "\n",
    "def map_index_and_pad(txt, **kwargs):\n",
    "    \"\"\"Mapeia índices e realiza padding/truncamento\"\"\"\n",
    "    \n",
    "    # Recupera argumentos\n",
    "    max_lenth = kwargs['max_length']\n",
    "    k2i = kwargs['key2index']\n",
    "\n",
    "    # Transforma tokens em indices\n",
    "    indices = [k2i[t] for t in txt if t in k2i.keys()]\n",
    "    \n",
    "    # Aplica padding ou truncamento\n",
    "    n_tokens = len(indices)\n",
    "\n",
    "    # O padding vai indicar um índice ao final da matrix de \n",
    "    # vetores densos que contem um vetor n-dinmensional preenchido\n",
    "    # com zeros\n",
    "    padded_index = len(k2i)\n",
    "    \n",
    "    # Truncamento\n",
    "    if n_tokens > max_lenth:\n",
    "        seq_indices = indices[:max_lenth]\n",
    "\n",
    "    # Padding\n",
    "    else:\n",
    "        padding = max_lenth - n_tokens\n",
    "        seq_indices = list(np.repeat(padded_index, padding))\n",
    "        seq_indices.extend(indices)\n",
    "    \n",
    "    # Retorna indices como uma string\n",
    "    return np.array(seq_indices)\n",
    "        \n",
    "\n",
    "# Mapeia ngrams para indíces e realiza padding ou truncamento quando\n",
    "# necessário\n",
    "imdb_reviews['seqs'] = imdb_reviews['ngrams'].apply(\n",
    "    map_index_and_pad, \n",
    "    max_length=130, \n",
    "    key2index=w2v_model.wv.key_to_index\n",
    ")\n",
    "\n",
    "# Exportando dataset pré-processado para eventual leitura\n",
    "with open(\"pkl/imdb_reviews_preprocessed.pkl\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(imdb_reviews))\n",
    "\n",
    "# Apresenta primeiras linhas\n",
    "imdb_reviews.head()\n"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "      filename  usage sent                                               text  \\\n",
       "0    494_9.txt  train  pos  This is the start of a new and interesting Sta...   \n",
       "1  11293_3.txt  train  neg  This movie was a real torture fest to sit thro...   \n",
       "2   6429_1.txt   test  neg  What can be said about such a pathetic movie ?...   \n",
       "3   9913_8.txt   test  pos  Kirk and the crew are visiting a federation mi...   \n",
       "4   4421_4.txt  train  neg  The subject is certainly compelling: a group o...   \n",
       "\n",
       "                                          text_clean  \\\n",
       "0  start new interest star trek series earth kind...   \n",
       "1  movie real torture fest sit first mistake trea...   \n",
       "2  say pathetic movie bad act main actress seem k...   \n",
       "3  kirk crew visit federation mine colony remote ...   \n",
       "4  subject certainly compel group people take lov...   \n",
       "\n",
       "                                              ngrams  n_tokens  \\\n",
       "0  [start, new, interest, star_trek_series, earth...        71   \n",
       "1  [movie, real, torture, fest, sit, first, mista...       271   \n",
       "2  [say, pathetic, movie, bad, act, main_actress,...       209   \n",
       "3  [kirk_crew, visit, federation, mine, colony, r...        93   \n",
       "4  [subject, certainly, compel, group, people, ta...       123   \n",
       "\n",
       "                                                seqs  \n",
       "0  [64360, 64360, 64360, 64360, 64360, 64360, 643...  \n",
       "1  [1, 67, 832, 3742, 344, 40, 836, 629, 14360, 9...  \n",
       "2  [21, 853, 1, 25, 27, 14564, 35, 19, 2, 8468, 6...  \n",
       "3  [64360, 64360, 64360, 64360, 64360, 64360, 643...  \n",
       "4  [64360, 64360, 64360, 64360, 64360, 64360, 643...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>usage</th>\n",
       "      <th>sent</th>\n",
       "      <th>text</th>\n",
       "      <th>text_clean</th>\n",
       "      <th>ngrams</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>seqs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>494_9.txt</td>\n",
       "      <td>train</td>\n",
       "      <td>pos</td>\n",
       "      <td>This is the start of a new and interesting Sta...</td>\n",
       "      <td>start new interest star trek series earth kind...</td>\n",
       "      <td>[start, new, interest, star_trek_series, earth...</td>\n",
       "      <td>71</td>\n",
       "      <td>[64360, 64360, 64360, 64360, 64360, 64360, 643...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11293_3.txt</td>\n",
       "      <td>train</td>\n",
       "      <td>neg</td>\n",
       "      <td>This movie was a real torture fest to sit thro...</td>\n",
       "      <td>movie real torture fest sit first mistake trea...</td>\n",
       "      <td>[movie, real, torture, fest, sit, first, mista...</td>\n",
       "      <td>271</td>\n",
       "      <td>[1, 67, 832, 3742, 344, 40, 836, 629, 14360, 9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6429_1.txt</td>\n",
       "      <td>test</td>\n",
       "      <td>neg</td>\n",
       "      <td>What can be said about such a pathetic movie ?...</td>\n",
       "      <td>say pathetic movie bad act main actress seem k...</td>\n",
       "      <td>[say, pathetic, movie, bad, act, main_actress,...</td>\n",
       "      <td>209</td>\n",
       "      <td>[21, 853, 1, 25, 27, 14564, 35, 19, 2, 8468, 6...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9913_8.txt</td>\n",
       "      <td>test</td>\n",
       "      <td>pos</td>\n",
       "      <td>Kirk and the crew are visiting a federation mi...</td>\n",
       "      <td>kirk crew visit federation mine colony remote ...</td>\n",
       "      <td>[kirk_crew, visit, federation, mine, colony, r...</td>\n",
       "      <td>93</td>\n",
       "      <td>[64360, 64360, 64360, 64360, 64360, 64360, 643...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4421_4.txt</td>\n",
       "      <td>train</td>\n",
       "      <td>neg</td>\n",
       "      <td>The subject is certainly compelling: a group o...</td>\n",
       "      <td>subject certainly compel group people take lov...</td>\n",
       "      <td>[subject, certainly, compel, group, people, ta...</td>\n",
       "      <td>123</td>\n",
       "      <td>[64360, 64360, 64360, 64360, 64360, 64360, 643...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "metadata": {},
     "execution_count": 13
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "\n",
    "# Exemplo de mapeamento de tokens para índices e padding/truncamento\n",
    "print(f\"\\nFrase original:\\n{imdb_reviews['ngrams'][0]}\")\n",
    "padded = map_index_and_pad(\n",
    "    imdb_reviews['ngrams'][0], \n",
    "    max_length=130, \n",
    "    key2index=w2v_model.wv.key_to_index\n",
    ")\n",
    "print(f\"\\nMapeamento de índices e aplicação de Padding/Truncamento:\\n{padded}\\n\")\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Frase original:\n",
      "['start', 'new', 'interest', 'star_trek_series', 'earth', 'kind', 'feel', 'darker', 'less', 'plaggy', 'scenography', 'character', 'need', 'time', 'develop', 'potential', 'one', 'thing', 'fairly', 'disappoint', 'star_trek_series', 'really', 'portray', 'gloomy', 'picture', 'equality', 'men_women', 'future', 'paint', 'positive', 'picture', 'everything_else', 'earth', 'stop', 'war', 'famine', 'etc', 'female_character', 'two', 'subcommander', 'pol', 'vulcan', 'communications', 'officer', 'hoshi', 'human', 'hoshi', 'quite', 'wimpy', 'pol', 'make', 'vulcan', 'babe', 'crew', 'attitudes', 'feel', 'bite', 'american', 'oppose', 'international', 'feel', 'tng', 'crew', 'create', 'interest', 'dynamics', 'good', 'pilot', 'though', 'good', 'series']\n",
      "\n",
      "Mapeamento de índices e aplicação de Padding/Truncamento:\n",
      "[64360 64360 64360 64360 64360 64360 64360 64360 64360 64360 64360 64360\n",
      " 64360 64360 64360 64360 64360 64360 64360 64360 64360 64360 64360 64360\n",
      " 64360 64360 64360 64360 64360 64360 64360 64360 64360 64360 64360 64360\n",
      " 64360 64360 64360 64360 64360 64360 64360 64360 64360 64360 64360 64360\n",
      " 64360 64360 64360 64360 64360 64360 64360 64360 64360 64360 64360 64360\n",
      "    69   124    54 21337   521   106    52  4147   245 57668    10    89\n",
      "    12   557   779     2    64   821   289 21337    14   304  6385   225\n",
      " 11283  4211   564   930  1442   225  1961   521   283   260 17137   381\n",
      "  4555    47 58597 23235 13980 15307  1726 45949   337 45949    88 11838\n",
      " 23235     3 13980  3873   775  3653    52   120   249  2012  1949    52\n",
      " 13705   775   234    54  7124     7  1275    79     7   123]\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Definição dos conjuntos de treino, validação e teste\n",
    "---\n",
    "\n",
    "O processo de treinamento da rede LSTM é realizado em batches, ou seja, um conjunto frases (sequência de tokens) é entregue ao modelo de cada vez. Os batches são construídos a partir da divisão do dataset de treinamento em partições. Dentro de cada batch, é importante que todas as frases tenham o mesmo número de tokens, visto que esse será o número de células (time steps) criadas para cada camada LSTM do modelo.\n",
    "\n",
    "Para validação e otimização dos hiperparâmetros das redes neurais se faz necessária a implementação de um processo de validação cruzada, que nos permite avaliar a acurácia atingida pela rede neural com diferentes conjuntos de hiperparâmetros.\n",
    "\n",
    "Para tanto, será utilizado um processo de validação cruzada hold-out. Nesta metodologia 35% dos dados serão utilizados para ajuste dos pesos da rede (treino), enquanto 15% servirão como conjunto de validação para otimização de hiperparâmetros. Os 50% restantes serão utilizados como conjunto de teste para avaliar a acurácia do modelo.\n",
    "\n",
    "Normalmente o hold-out é realizado com proporções como 70 % treino e 30% teste, porém neste caso utilizamos as proporções pré-estabelecidas dentro do dataset pela coluna \"usage\".\n",
    "\n",
    "Para tanto, foram definidas a classe `ReviewsDataset()` e a função `get_train_val_test()`. A classe e a função dividirão o dataset em conjuntos de treino, validação e teste. A função ainda realiza a transformação dos conjuntos em iteradores em forma de batches com o auxílio da Clase `DataLoader()` do PyTorch.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "\n",
    "class ReviewsDataset(Dataset):\n",
    "    \"\"\"Classe base para a criação dos iteradores de dados para\n",
    "    treinamento, validação e teste da rede LSTM\n",
    "\n",
    "    Argumentos\n",
    "    ----------\n",
    "\n",
    "        X (np.array)\n",
    "            Sequências de índices para alimentação da rede LSTM\n",
    "        \n",
    "        y (np.array)\n",
    "            Labels de cada sequência\n",
    "            \n",
    "    \"\"\"\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "\n",
    "def get_train_val_test(df, val_perc=0.3, batch_size=500):\n",
    "    \"\"\"Retorna iteradores para os processos de treinamento, validação e \n",
    "    teste da rede LSTM\n",
    "    \"\"\"\n",
    "    # Mapeia labels (neg=0, pos=1)\n",
    "    df['label'] = df['sent'].apply(\n",
    "        lambda y: 0 if y == 'neg' else 1\n",
    "    )\n",
    "\n",
    "    # Separa dataset de treino e teste\n",
    "    train_X, X_test = df[df.usage == 'train'], df[df.usage == 'test']\n",
    "    train_y, y_test = train_X['label'].values, X_test['label'].values\n",
    "\n",
    "    # Separa dataset de treino e validação\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        train_X,\n",
    "        train_y,\n",
    "        test_size=val_perc\n",
    "    )\n",
    "\n",
    "    # Recupera frases\n",
    "    X_train = X_train.seqs.values\n",
    "    X_val = X_val.seqs.values\n",
    "    X_test = X_test.seqs.values\n",
    "\n",
    "    # Cria datasets usando classe ReviewsDataset \n",
    "    ds_train = ReviewsDataset(list(X_train), list(y_train))\n",
    "    ds_val = ReviewsDataset(list(X_val), list(y_val))\n",
    "    ds_test = ReviewsDataset(list(X_test), list(y_test))\n",
    "\n",
    "    # Constrói iteradores para alimentar a rede\n",
    "    train_iter = DataLoader(ds_train, batch_size=batch_size)\n",
    "    val_iter = DataLoader(ds_val, batch_size=batch_size)\n",
    "    test_iter = DataLoader(ds_test, batch_size=batch_size)\n",
    "\n",
    "    # Retorna iteradores\n",
    "    return train_iter, val_iter, test_iter\n",
    "\n",
    "\n",
    "# Recuperando iteradores para o treinamento, validação e teste da rede\n",
    "train_dl, val_dl, test_dl = get_train_val_test(\n",
    "    df=imdb_reviews,\n",
    "    val_perc=0.3, \n",
    "    batch_size=500\n",
    ")\n",
    "\n",
    "# Exportando dataset de treino para eventual leitura\n",
    "with open(\"pkl/train_dl.pkl\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(train_dl))\n",
    "\n",
    "# Exportando dataset de validação para eventual leitura\n",
    "with open(\"pkl/val_dl.pkl\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(val_dl))\n",
    "\n",
    "# Exportando dataset de teste para eventual leitura\n",
    "with open(\"pkl/test_dl.pkl\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(test_dl))\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Implementação de Rede Neural LSTM\n",
    "---\n",
    "\n",
    "A rede neural LSTM será implementada na forma da classe `SentAnalysis_LSTM()`. A classe herda funcionalidades do módulo nn.Module do PyTorch.\n",
    "\n",
    "A rede se trata de uma arquitetura do tipo recorrente com células LSTM seguidas de uma camada de dropout e uma camada totalmente conectada para predição da classe (sentimento) da revisão.\n",
    "\n",
    "A rede está estruturada da seguinte maneira:\n",
    "\n",
    "- Camada de Embedding com vetores treinados com modelo Word2Vec;\n",
    "- Camada Recorrente LSTM com inicialização Xavier;\n",
    "- Camada Dropout com manutenção de 50% dos neurônios;\n",
    "- Camada totalmente conectada com ativação sigmoidal.\n",
    "\n",
    "Variáveis como o número de camadas LSTM, número de neurônios por célula LSTM e threshold para classificação serão determinados durante a etapa de otimização de hiperparâmetros.\n",
    "\n",
    "A rede foi mantida em tamanho reduzido para não extender o processo de treinamento da rede e otimização dos hiperparâmetros."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "\n",
    "# Implementação da rede LSTM para Análise de Sentimento\n",
    "class SentAnalysis_LSTM(nn.Module):\n",
    "    \"\"\"Rede LSTM para Análise de Sentimento\n",
    "\n",
    "    https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "\n",
    "    Argumentos\n",
    "    ----------\n",
    "\n",
    "        n_hidden (int):\n",
    "            Número de neurônios em cada célula LSTM\n",
    "        \n",
    "        n_layers (int):\n",
    "            Número de camadas LSTM\n",
    "        \n",
    "        k2i (dictionary):\n",
    "            Dicionário com vocabulário para mapeamento entre token\n",
    "            e índice na camada de embedding\n",
    "        \n",
    "        weights_matrix (torch.tensor):\n",
    "            Matriz, no formato torch.tensor, contendo vetores densos\n",
    "            para mapeamento dos tokens\n",
    "        \n",
    "        threshold (float):\n",
    "            Threshold para classificação entre positivo e negativo\n",
    "\n",
    "\n",
    "    Métodos Públicos\n",
    "    ----------------\n",
    "\n",
    "        load():\n",
    "            Carrega pesos do modelo salvos como um state_dict no formato\n",
    "            '.pth'\n",
    "\n",
    "        predict():\n",
    "            Realiza predição de uma nova review\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Compila regex para caracteres\n",
    "    _CHAR_REGEX = re.compile(r'[^A-Za-z\\s]')\n",
    "    \n",
    "    def __init__(self, n_hidden, n_layers, k2i, weights_matrix, threshold):\n",
    "        super(SentAnalysis_LSTM, self).__init__()\n",
    "\n",
    "        # Define atributos da instância\n",
    "        self.hidden_dim = n_hidden\n",
    "        self.lstm_n_layers = n_layers\n",
    "        self._stopwords = stopwords.words('english')\n",
    "        self._k2i = k2i\n",
    "        self.threshold = threshold\n",
    "        self._lem = WordNetLemmatizer()\n",
    "\n",
    "        # Define camada de Embedding (o método from_pretrained faz um\n",
    "        # \"freeze\" na atualização dos pesos do layer de embedding)\n",
    "        self.emb = nn.Embedding.from_pretrained(weights_tensor)\n",
    "\n",
    "        # Define camada LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=weights_matrix.size()[1],\n",
    "            hidden_size=n_hidden,\n",
    "            num_layers=n_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # Define camada de Dropout\n",
    "        self.dropout = nn.Dropout(p=0.5)\n",
    "        \n",
    "        # Define Camada Totalmente Conectada\n",
    "        self.fc = nn.Linear(n_hidden, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Inicializa pesos das células e camadas LSTM utilizando a \n",
    "        # inicialização Xavier\n",
    "        h = torch.zeros((self.lstm_n_layers, x.size(0), self.hidden_dim))\n",
    "        c = torch.zeros((self.lstm_n_layers, x.size(0), self.hidden_dim))\n",
    "        torch.nn.init.xavier_normal_(h)\n",
    "        torch.nn.init.xavier_normal_(c)\n",
    "\n",
    "        # Cada frase passa pela camada de embedding\n",
    "        embedded = self.emb(x)\n",
    "\n",
    "        # Os vetores densos são então entregues a rede LSTM definida\n",
    "        out, _ = self.lstm(embedded, (h,c))\n",
    "\n",
    "        # Aplica camada de dropout\n",
    "        dropped = self.dropout(out[:,-1,:])\n",
    "\n",
    "        # O output é entregue a uma camada totalmente conectada com uma \n",
    "        # ativação sigmoid ao final\n",
    "        prob = torch.sigmoid(self.fc(dropped))\n",
    "\n",
    "        # A probabilidade final é então retornada\n",
    "        return prob\n",
    "    \n",
    "    def load(self, model_path):\n",
    "        \"\"\"Carrega um modelo treinado\"\"\"\n",
    "        self.load_state_dict(torch.load(model_path))\n",
    "        self.eval()\n",
    "\n",
    "    def predict(self, phrase, bigrams, trigrams):\n",
    "        \"\"\"Realiza predições\"\"\"\n",
    "\n",
    "        # Checa se objeto é uma string\n",
    "        if not type(phrase) == str:\n",
    "            raise ValueError('Passe uma string como parâmetro!')\n",
    "\n",
    "        # Limpeza de caracteres não-relevantes\n",
    "        phrase = self._sentence_cleaning(phrase)\n",
    "\n",
    "        # União de palavras compostas\n",
    "        phrase = trigrams[bigrams[phrase.split()]]\n",
    "\n",
    "        # Recupera índices para matriz de embedding\n",
    "        phrase = self._map_index(phrase)\n",
    "\n",
    "        # Se frase não possuir nenhum tokens no vocabulário\n",
    "        # retorna resultado aleatorio\n",
    "        if len(phrase) == 0:\n",
    "            pred = random.choice([0, 1])\n",
    "\n",
    "        else:\n",
    "            # Realiza predição\n",
    "            phrase = torch.tensor([phrase,], dtype=torch.float32)\n",
    "            probs = self(phrase.long())\n",
    "            preds = torch.where(probs >= self.threshold, 1, 0)\n",
    "            pred = preds.detach().numpy()[0][0]\n",
    "            \n",
    "        # Retorna resultado como np.array\n",
    "        return pred\n",
    "    \n",
    "    def _sentence_cleaning(self, txt:str):\n",
    "        \"\"\"Realiza limpeza das frases\"\"\"\n",
    "\n",
    "        # Remove tags HTML\n",
    "        txt = BeautifulSoup(txt, \"html.parser\")\n",
    "        txt = txt.get_text()\n",
    "\n",
    "        # Recupera apenas letras\n",
    "        txt = self._CHAR_REGEX.sub(\" \", txt)\n",
    "\n",
    "        # Passa todos os caracteres para letras minúsculas\n",
    "        txt = txt.lower().strip()\n",
    "\n",
    "        # Cria tokens\n",
    "        txt = word_tokenize(txt, \"english\")\n",
    "\n",
    "        # Realiza lemmatize para cada palavra\n",
    "        txt = [self._lem.lemmatize(w, pos='v') for w in txt]\n",
    "\n",
    "        # Remove stopwords\n",
    "        txt = [w for w in txt if w not in self._stopwords]\n",
    "        \n",
    "        # Retorna texto\n",
    "        return ' '.join(txt)\n",
    "    \n",
    "    def _map_index(self, txt):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        # Transforma tokens em indices\n",
    "        indices = [self._k2i[t] for t in txt if t in self._k2i.keys()]\n",
    "        \n",
    "        # Retorna indices como uma string\n",
    "        return np.array(indices)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Treinamento e otimização de hiperparâmetros\n",
    "---\n",
    "\n",
    "O experimento implementado abaixo teve como intuito otimizar os hiperparâmetros utilizados para o treinamento da rede `SentAnalysis_LSTM()`, procurando elucidar como mudanças no número de camadas LSTM, número de neurônios, learning rate, épocas e threshold afetariam a acurácia da classificação.\n",
    "\n",
    "O processo de otimização foi realizado de forma simplificada com auxílio da biblioteca `optuna`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "\n",
    "def tune_hyperparams(trial):\n",
    "\n",
    "    # Define hiperparâmetros para o trial\n",
    "    lr = trial.suggest_loguniform('lr', 1e-4, 1e-1)\n",
    "    epochs = trial.suggest_int('epochs', 100, 300, 50)\n",
    "    threshold = trial.suggest_float('threshold', 0.4, 0.6)\n",
    "    n_hidden = trial.suggest_int('n_hidden', 30, 90)\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "\n",
    "    # Instancia modelo\n",
    "    lstm_model = SentAnalysis_LSTM(\n",
    "        n_hidden=n_hidden,\n",
    "        n_layers=n_layers,\n",
    "        k2i=w2v_model.wv.key_to_index,\n",
    "        weights_matrix=weights_tensor,\n",
    "        threshold=threshold\n",
    "    )\n",
    "    \n",
    "    # Inicializa otimizador e critério para calculo da loss\n",
    "    optimizer = torch.optim.Adam(lstm_model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Ativa modo de treinamento\n",
    "    lstm_model.train()\n",
    "    \n",
    "    # Loop Treinamento\n",
    "    for i in range(epochs):\n",
    "        \n",
    "        # Itera sobre batches\n",
    "        for x, y in train_dl:\n",
    "\n",
    "            # Forward pass\n",
    "            x = x.long()\n",
    "            y_hat = lstm_model(x)\n",
    "            y_pred = torch.where(y_hat >= 0.5, 1, 0)\n",
    "\n",
    "            # Cálculo da Loss e Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(y_hat, y.reshape(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    # Inicializa contadores\n",
    "    correct, total = 0, 0\n",
    "    \n",
    "    # Ativa modo de evaluation\n",
    "    lstm_model.eval()\n",
    "\n",
    "    # Loop Validação\n",
    "    for x, y in val_dl:\n",
    "\n",
    "        # Predição\n",
    "        x = x.long()\n",
    "        y_hat = lstm_model(x)\n",
    "        y_pred = torch.where(y_hat >= threshold, 1, 0)\n",
    "\n",
    "        # Cálculo da loss\n",
    "        loss = criterion(y_hat, y.reshape(-1, 1))\n",
    "        \n",
    "        # Atualização dos contadores\n",
    "        correct += (y_pred == y.reshape(-1, 1)).float().sum()\n",
    "        total += y.shape[0]\n",
    "        \n",
    "    # Retorna acurácia de validação\n",
    "    return correct/total\n",
    "\n",
    "\n",
    "# Criação e execução de experimento para maximização da acurácia sobre\n",
    "# o conjunto de validação\n",
    "hyper_optim = optuna.create_study(direction=\"maximize\")\n",
    "hyper_optim.optimize(tune_hyperparams, n_trials=40, n_jobs=2)\n",
    "\n",
    "# Exportando resultados para eventual leitura\n",
    "with open(\"pkl/hyper_optim_study.pkl\", \"wb\") as f:\n",
    "    f.write(pickle.dumps(hyper_optim))\n",
    "\n",
    "# Apresenta hiperparâmetros de melhor performance\n",
    "print(f\"\\n\\nHiperparâmetros otimizados pelo Optuna: \\n{hyper_optim.best_params}\")\n",
    "\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\u001b[32m[I 2021-07-24 16:30:05,642]\u001b[0m A new study created in memory with name: no-name-139fd3de-279a-4919-aef8-af11649e324a\u001b[0m\n",
      "/home/marcus/miniconda3/envs/neural-nets/lib/python3.8/site-packages/optuna/study.py:394: FutureWarning: `n_jobs` argument has been deprecated in v2.7.0. This feature will be removed in v4.0.0. See https://github.com/optuna/optuna/releases/tag/v2.7.0.\n",
      "  warnings.warn(\n",
      "\u001b[32m[I 2021-07-24 17:12:08,307]\u001b[0m Trial 0 finished with value: 0.8199999928474426 and parameters: {'lr': 0.008779254466196871, 'epochs': 150, 'threshold': 0.4226108018591449, 'n_hidden': 76, 'n_layers': 1}. Best is trial 0 with value: 0.8199999928474426.\u001b[0m\n",
      "\u001b[32m[I 2021-07-24 20:27:56,615]\u001b[0m Trial 1 finished with value: 0.8464000225067139 and parameters: {'lr': 0.04465223730723718, 'epochs': 300, 'threshold': 0.4665918326390188, 'n_hidden': 39, 'n_layers': 3}. Best is trial 1 with value: 0.8464000225067139.\u001b[0m\n",
      "\u001b[32m[I 2021-07-24 20:32:35,244]\u001b[0m Trial 2 finished with value: 0.8425333499908447 and parameters: {'lr': 0.026854778829423238, 'epochs': 250, 'threshold': 0.4701923157777295, 'n_hidden': 54, 'n_layers': 3}. Best is trial 1 with value: 0.8464000225067139.\u001b[0m\n",
      "\u001b[32m[I 2021-07-24 21:32:22,853]\u001b[0m Trial 3 finished with value: 0.8186666369438171 and parameters: {'lr': 0.017509223349551527, 'epochs': 250, 'threshold': 0.4508340924566926, 'n_hidden': 70, 'n_layers': 1}. Best is trial 1 with value: 0.8464000225067139.\u001b[0m\n",
      "\u001b[32m[I 2021-07-24 22:03:09,558]\u001b[0m Trial 5 finished with value: 0.8562666773796082 and parameters: {'lr': 0.0003668218819861578, 'epochs': 100, 'threshold': 0.4342032336481494, 'n_hidden': 37, 'n_layers': 2}. Best is trial 5 with value: 0.8562666773796082.\u001b[0m\n",
      "\u001b[32m[I 2021-07-24 22:36:57,795]\u001b[0m Trial 4 finished with value: 0.8353333473205566 and parameters: {'lr': 0.0012699997595563698, 'epochs': 150, 'threshold': 0.47737667229481595, 'n_hidden': 74, 'n_layers': 3}. Best is trial 5 with value: 0.8562666773796082.\u001b[0m\n",
      "\u001b[32m[I 2021-07-24 23:12:35,254]\u001b[0m Trial 6 finished with value: 0.8352000117301941 and parameters: {'lr': 0.018910582645507545, 'epochs': 300, 'threshold': 0.4980911180580514, 'n_hidden': 64, 'n_layers': 1}. Best is trial 5 with value: 0.8562666773796082.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 00:03:54,950]\u001b[0m Trial 7 finished with value: 0.7793333530426025 and parameters: {'lr': 0.01553456079739286, 'epochs': 250, 'threshold': 0.4697667362363398, 'n_hidden': 87, 'n_layers': 1}. Best is trial 5 with value: 0.8562666773796082.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 00:08:57,547]\u001b[0m Trial 8 finished with value: 0.8471999764442444 and parameters: {'lr': 0.00011165159158251207, 'epochs': 250, 'threshold': 0.5619810974745543, 'n_hidden': 68, 'n_layers': 1}. Best is trial 5 with value: 0.8562666773796082.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 01:18:52,488]\u001b[0m Trial 10 finished with value: 0.8430666923522949 and parameters: {'lr': 0.00033318297223326564, 'epochs': 300, 'threshold': 0.5261438866497337, 'n_hidden': 60, 'n_layers': 1}. Best is trial 5 with value: 0.8562666773796082.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 01:45:35,881]\u001b[0m Trial 11 finished with value: 0.8564000129699707 and parameters: {'lr': 0.0011095639603373702, 'epochs': 100, 'threshold': 0.4064876336844557, 'n_hidden': 31, 'n_layers': 2}. Best is trial 11 with value: 0.8564000129699707.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 02:05:22,972]\u001b[0m Trial 12 finished with value: 0.8574666380882263 and parameters: {'lr': 0.0009649112798970134, 'epochs': 100, 'threshold': 0.40744376720848974, 'n_hidden': 32, 'n_layers': 2}. Best is trial 12 with value: 0.8574666380882263.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 02:29:54,224]\u001b[0m Trial 13 finished with value: 0.8422666788101196 and parameters: {'lr': 0.0021891163127413754, 'epochs': 100, 'threshold': 0.40192036875627674, 'n_hidden': 30, 'n_layers': 2}. Best is trial 12 with value: 0.8574666380882263.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 03:16:22,282]\u001b[0m Trial 14 finished with value: 0.8445333242416382 and parameters: {'lr': 0.0006674432526068002, 'epochs': 150, 'threshold': 0.4020688045074908, 'n_hidden': 48, 'n_layers': 2}. Best is trial 12 with value: 0.8574666380882263.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 03:39:43,170]\u001b[0m Trial 15 finished with value: 0.8569333553314209 and parameters: {'lr': 0.0038617775155529686, 'epochs': 100, 'threshold': 0.40324366974043924, 'n_hidden': 30, 'n_layers': 2}. Best is trial 12 with value: 0.8574666380882263.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 03:42:24,537]\u001b[0m Trial 9 finished with value: 0.8038666844367981 and parameters: {'lr': 0.022503044262531118, 'epochs': 250, 'threshold': 0.44658498564063337, 'n_hidden': 63, 'n_layers': 3}. Best is trial 12 with value: 0.8574666380882263.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 04:29:53,011]\u001b[0m Trial 17 finished with value: 0.8378666639328003 and parameters: {'lr': 0.005386820181024234, 'epochs': 150, 'threshold': 0.5962529434904242, 'n_hidden': 44, 'n_layers': 2}. Best is trial 12 with value: 0.8574666380882263.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 04:29:53,952]\u001b[0m Trial 16 finished with value: 0.8137333393096924 and parameters: {'lr': 0.0031086760461066, 'epochs': 150, 'threshold': 0.5886034730542544, 'n_hidden': 44, 'n_layers': 2}. Best is trial 12 with value: 0.8574666380882263.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 05:16:15,683]\u001b[0m Trial 18 finished with value: 0.8329333066940308 and parameters: {'lr': 0.0034747796279580912, 'epochs': 200, 'threshold': 0.41738959296901595, 'n_hidden': 30, 'n_layers': 2}. Best is trial 12 with value: 0.8574666380882263.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 05:16:41,767]\u001b[0m Trial 19 finished with value: 0.857200026512146 and parameters: {'lr': 0.00012039127512940795, 'epochs': 200, 'threshold': 0.41895297242303187, 'n_hidden': 30, 'n_layers': 2}. Best is trial 12 with value: 0.8574666380882263.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 06:40:10,013]\u001b[0m Trial 20 finished with value: 0.6060000061988831 and parameters: {'lr': 0.09064983134820143, 'epochs': 100, 'threshold': 0.5324391465256195, 'n_hidden': 37, 'n_layers': 2}. Best is trial 12 with value: 0.8574666380882263.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 07:06:37,473]\u001b[0m Trial 21 finished with value: 0.8532000184059143 and parameters: {'lr': 0.00010835602761971888, 'epochs': 200, 'threshold': 0.5241820145559081, 'n_hidden': 53, 'n_layers': 3}. Best is trial 12 with value: 0.8574666380882263.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 07:26:44,038]\u001b[0m Trial 22 finished with value: 0.853866696357727 and parameters: {'lr': 0.00011816511901743727, 'epochs': 200, 'threshold': 0.43048909863038837, 'n_hidden': 33, 'n_layers': 2}. Best is trial 12 with value: 0.8574666380882263.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 07:53:11,056]\u001b[0m Trial 23 finished with value: 0.853600025177002 and parameters: {'lr': 0.0001937752059534381, 'epochs': 200, 'threshold': 0.42721852636694424, 'n_hidden': 34, 'n_layers': 2}. Best is trial 12 with value: 0.8574666380882263.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 08:15:09,789]\u001b[0m Trial 24 finished with value: 0.8582666516304016 and parameters: {'lr': 0.0002690289357362579, 'epochs': 200, 'threshold': 0.4010815756043639, 'n_hidden': 36, 'n_layers': 2}. Best is trial 24 with value: 0.8582666516304016.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 08:25:04,840]\u001b[0m Trial 25 finished with value: 0.8425333499908447 and parameters: {'lr': 0.007128405978281139, 'epochs': 100, 'threshold': 0.41409024655302135, 'n_hidden': 43, 'n_layers': 2}. Best is trial 24 with value: 0.8582666516304016.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 09:26:48,047]\u001b[0m Trial 26 finished with value: 0.850933313369751 and parameters: {'lr': 0.0002897045935264019, 'epochs': 200, 'threshold': 0.4456742254355075, 'n_hidden': 47, 'n_layers': 2}. Best is trial 24 with value: 0.8582666516304016.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 09:30:10,733]\u001b[0m Trial 27 finished with value: 0.8526666760444641 and parameters: {'lr': 0.0003824992955204425, 'epochs': 200, 'threshold': 0.4423677887516976, 'n_hidden': 50, 'n_layers': 2}. Best is trial 24 with value: 0.8582666516304016.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 10:34:22,295]\u001b[0m Trial 29 finished with value: 0.8507999777793884 and parameters: {'lr': 0.0007233609872425946, 'epochs': 150, 'threshold': 0.4902656167106776, 'n_hidden': 39, 'n_layers': 3}. Best is trial 24 with value: 0.8582666516304016.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 10:52:54,008]\u001b[0m Trial 28 finished with value: 0.8485333323478699 and parameters: {'lr': 0.0005667548102692265, 'epochs': 200, 'threshold': 0.49467407895306487, 'n_hidden': 39, 'n_layers': 3}. Best is trial 24 with value: 0.8582666516304016.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 11:18:52,430]\u001b[0m Trial 30 finished with value: 0.8497333526611328 and parameters: {'lr': 0.00018770824339759377, 'epochs': 150, 'threshold': 0.4206006686047111, 'n_hidden': 88, 'n_layers': 1}. Best is trial 24 with value: 0.8582666516304016.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 11:39:03,854]\u001b[0m Trial 31 finished with value: 0.8485333323478699 and parameters: {'lr': 0.00017980505568365127, 'epochs': 150, 'threshold': 0.41719053969253345, 'n_hidden': 90, 'n_layers': 1}. Best is trial 24 with value: 0.8582666516304016.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 11:43:46,523]\u001b[0m Trial 32 finished with value: 0.8442666530609131 and parameters: {'lr': 0.001623293186660559, 'epochs': 100, 'threshold': 0.4002787149924825, 'n_hidden': 30, 'n_layers': 2}. Best is trial 24 with value: 0.8582666516304016.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 12:02:29,362]\u001b[0m Trial 33 finished with value: 0.8482666611671448 and parameters: {'lr': 0.0016745125655386341, 'epochs': 100, 'threshold': 0.4029424886827741, 'n_hidden': 34, 'n_layers': 2}. Best is trial 24 with value: 0.8582666516304016.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 12:33:49,458]\u001b[0m Trial 34 finished with value: 0.8405333161354065 and parameters: {'lr': 0.0009598265802433948, 'epochs': 200, 'threshold': 0.40178735751753625, 'n_hidden': 35, 'n_layers': 2}. Best is trial 24 with value: 0.8582666516304016.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 13:08:16,234]\u001b[0m Trial 35 finished with value: 0.8377333283424377 and parameters: {'lr': 0.0009076141846839775, 'epochs': 250, 'threshold': 0.45568726946096993, 'n_hidden': 37, 'n_layers': 2}. Best is trial 24 with value: 0.8582666516304016.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 13:47:59,438]\u001b[0m Trial 36 finished with value: 0.8350666761398315 and parameters: {'lr': 0.00886521326249174, 'epochs': 250, 'threshold': 0.4530588780161301, 'n_hidden': 41, 'n_layers': 2}. Best is trial 24 with value: 0.8582666516304016.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 14:06:45,307]\u001b[0m Trial 37 finished with value: 0.831333339214325 and parameters: {'lr': 0.0032586903504465974, 'epochs': 200, 'threshold': 0.43398464120091, 'n_hidden': 41, 'n_layers': 2}. Best is trial 24 with value: 0.8582666516304016.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 14:23:48,752]\u001b[0m Trial 38 finished with value: 0.8562666773796082 and parameters: {'lr': 0.000495813431223099, 'epochs': 150, 'threshold': 0.4336018217218966, 'n_hidden': 30, 'n_layers': 2}. Best is trial 24 with value: 0.8582666516304016.\u001b[0m\n",
      "\u001b[32m[I 2021-07-25 14:45:44,641]\u001b[0m Trial 39 finished with value: 0.8528000116348267 and parameters: {'lr': 0.0004198901717430089, 'epochs': 150, 'threshold': 0.4140861529071317, 'n_hidden': 30, 'n_layers': 3}. Best is trial 24 with value: 0.8582666516304016.\u001b[0m\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "Hiperparâmetros otimizados pelo Optuna: \n",
      "{'lr': 0.0002690289357362579, 'epochs': 200, 'threshold': 0.4010815756043639, 'n_hidden': 36, 'n_layers': 2}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Com base nos hiperparâmetros definidos com o auxílio do optuna, podemos realizar o treinamento do modelo e salvar os pesos e sua estrutura completa para uso futuro "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "\n",
    "def train_model(model, train_dl, val_dl, epochs, lr):\n",
    "    \"\"\"Treinamento do Modelo\"\"\"\n",
    "\n",
    "    # Inicializa otimizador e critério para calculo da loss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.BCELoss()\n",
    "\n",
    "    # Inicializa listas para loss\n",
    "    train_losses, val_losses = [], []\n",
    "\n",
    "    # Itera sobre épocas\n",
    "    for i in range(epochs):\n",
    "\n",
    "        # Ativa modo de treinamento\n",
    "        model.train()\n",
    "\n",
    "        # Inicializa variável para armazenar loss da época\n",
    "        training_loss, validation_loss = 0, 0\n",
    "              \n",
    "        # Itera sobre batches para treinamento\n",
    "        for x, y in train_dl:\n",
    "\n",
    "            # Forward pass\n",
    "            x = x.long()\n",
    "            y_hat = model(x)\n",
    "\n",
    "            # Cálculo da Loss e Backpropagation\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(y_hat, y.reshape(-1, 1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Incremeta loss\n",
    "            training_loss += loss.item()\n",
    "        \n",
    "        # Ativa modo de evaluation\n",
    "        model.eval()\n",
    "        \n",
    "        # Itera sobre batches para validação\n",
    "        with torch.no_grad():\n",
    "            for x, y in val_dl:\n",
    "\n",
    "                # Predição\n",
    "                x = x.long()\n",
    "                y_hat = lstm_model(x)\n",
    "                \n",
    "                # Cálculo da loss\n",
    "                loss = criterion(y_hat, y.reshape(-1, 1))\n",
    "                \n",
    "                # Incremeta loss\n",
    "                validation_loss += loss.item()\n",
    "        \n",
    "        # Adiciona losses nas listas\n",
    "        train_losses.append(training_loss/len(train_dl))\n",
    "        val_losses.append(validation_loss/len(val_dl))\n",
    "    \n",
    "    # Apresenta gráfico com loss \n",
    "    plt.plot(train_losses, label='Loss Treinamento')\n",
    "    plt.plot(val_losses, label='Loss Validação')\n",
    "    plt.legend(frameon=False)\n",
    "    plt.xlabel('Épocas')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "\n",
    "# Instancia modelo LSTM\n",
    "lstm_model = SentAnalysis_LSTM(\n",
    "    n_hidden=hyper_optim.best_params['n_hidden'],\n",
    "    n_layers=hyper_optim.best_params['n_layers'],\n",
    "    k2i=w2v_model.wv.key_to_index,\n",
    "    weights_matrix=weights_tensor,\n",
    "    threshold=hyper_optim.best_params['threshold']\n",
    ")\n",
    "\n",
    "# Realiza treinamento do modelo\n",
    "train_model(\n",
    "    model=lstm_model,\n",
    "    train_dl=train_dl,\n",
    "    val_dl=val_dl,\n",
    "    epochs=hyper_optim.best_params['epochs'],\n",
    "    lr=hyper_optim.best_params['lr']\n",
    ")\n",
    "\n",
    "# Salva modelo para uso futuro (Pesos e Estrutura completa)\n",
    "torch.save(lstm_model.state_dict(), 'lstm_model_weights.pth')\n",
    "torch.save(lstm_model, 'lstm_model.pth')\n"
   ],
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Observando o gráfico acima podemos perceber que após, aproximadamente, 60 épocas a loss (erro) para o conjunto de validação apresenta leve incremento enquanto a loss para o conjunto de treinamento continua reduzindo. Este comportamento evidencia a ocorrência de overfitting, sendo o momento que o modelo começa a perder a sua capacidade de generalização e se ajusta demais às features do conjunto de treinamento. A seguir realizaremos a análise da acurácia sobre o conjunto de teste, para termos uma melhor ideia de como o modelo irá se comportar na classificação de revisões não utilizadas para o treinamento e otimização.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Análise de acurácia\n",
    "---\n",
    "\n",
    "Após realizarmos o treinamento do modelo e a otimização de seus hiperparâmetros, iremos apurar a sua acurácia sobre revisões não utilizadas em seu desenvolvimento."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "\n",
    "# Rodar modelo treinado sobre o conjunto de teste\n",
    "def test_metrics(model, train_dl, val_dl, test_dl):\n",
    "    \"\"\"\"\"\"\n",
    "\n",
    "    # Inicializa contadores\n",
    "    train_correct, train_total = 0, 0\n",
    "    val_correct, val_total = 0, 0\n",
    "    test_correct, test_total = 0, 0\n",
    "\n",
    "    # Define critério para cálculo da loss\n",
    "    criterion = nn.BCELoss()\n",
    "    \n",
    "    # Ativa modo de evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        # Loop Dados Treinamento\n",
    "        for x, y in train_dl:\n",
    "\n",
    "            # Predição\n",
    "            x = x.long()\n",
    "            y_hat = model(x)\n",
    "            y_pred = torch.where(y_hat >= model.threshold, 1, 0)\n",
    "\n",
    "            # Cálculo da loss\n",
    "            train_loss = criterion(y_hat, y.reshape(-1, 1))\n",
    "            \n",
    "            # Atualização dos contadores\n",
    "            train_correct += (y_pred == y.reshape(-1, 1)).float().sum()\n",
    "            train_total += y.shape[0]\n",
    "        \n",
    "        # Loop Dados Validação\n",
    "        for x, y in val_dl:\n",
    "\n",
    "            # Predição\n",
    "            x = x.long()\n",
    "            y_hat = model(x)\n",
    "            y_pred = torch.where(y_hat >= model.threshold, 1, 0)\n",
    "\n",
    "            # Cálculo da loss\n",
    "            val_loss = criterion(y_hat, y.reshape(-1, 1))\n",
    "            \n",
    "            # Atualização dos contadores\n",
    "            val_correct += (y_pred == y.reshape(-1, 1)).float().sum()\n",
    "            val_total += y.shape[0]\n",
    "\n",
    "        # Loop Dados Teste\n",
    "        for x, y in test_dl:\n",
    "\n",
    "            # Predição\n",
    "            x = x.long()\n",
    "            y_hat = model(x)\n",
    "            y_pred = torch.where(y_hat >= model.threshold, 1, 0)\n",
    "\n",
    "            # Cálculo da loss\n",
    "            test_loss = criterion(y_hat, y.reshape(-1, 1))\n",
    "            \n",
    "            # Atualização dos contadores\n",
    "            test_correct += (y_pred == y.reshape(-1, 1)).float().sum()\n",
    "            test_total += y.shape[0]\n",
    "    \n",
    "    # Calcula acurácias\n",
    "    acc_train = (train_correct/train_total).detach().numpy()\n",
    "    acc_val = (val_correct/val_total).detach().numpy()\n",
    "    acc_test = (test_correct/test_total).detach().numpy()\n",
    "\n",
    "    # Cria dataframe e apresenta dados\n",
    "    metrics = {\n",
    "        'Conjunto': ['Treino', 'Validação', 'Teste'], \n",
    "        'Acurácia': [acc_train, acc_val, acc_test], \n",
    "        'Loss': [train_loss.item(), val_loss.item(), test_loss.item()]\n",
    "    }\n",
    "\n",
    "    # Apresenta acurácia e loss\n",
    "    print('Acurácia e Loss para Conjuntos de Treino, Validação e Teste:\\n')\n",
    "    print(pd.DataFrame(metrics))\n",
    "\n",
    "\n",
    "# Apresenta métricas de treino, validação e teste\n",
    "test_metrics(lstm_model, train_dl, val_dl, test_dl)\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Acurácia e Loss para Conjuntos de Treino, Validação e Teste:\n",
      "\n",
      "    Conjunto   Acurácia      Loss\n",
      "0     Treino  0.9074857  0.210192\n",
      "1  Validação     0.8436  0.336035\n",
      "2      Teste    0.84584  0.442359\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "Novamente fica clara a ocorrência de overfitting na classificação realizada pelo modelo. Neste caso o overfitting está evidenciado não apenas pela loss (erro) do conjunto de validação, mas também pelas acurácias (porcentegem de acertos) de validação e teste. Podemos perceber que a acurácia de treino é superior as acurácias de validação e teste.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Carregando o modelo treinado e realizando classificações\n",
    "---\n",
    "\n",
    "As células abaixo permitem carregar o modelo treinado e realizar a classificação de novas revisões por meio do método `predict()`\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "\n",
    "# Leitura da matriz de vetores densos\n",
    "with open(\"pkl/bigrams.pkl\", \"rb\") as f:\n",
    "    bigrams = pickle.loads(f.read())\n",
    "\n",
    "# Leitura da matriz de vetores densos\n",
    "with open(\"pkl/trigrams.pkl\", \"rb\") as f:\n",
    "    trigrams = pickle.loads(f.read())\n",
    "\n",
    "# Carrega modelo treinado\n",
    "lstm_model = torch.load('lstm_model.pth')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "\n",
    "# Insere Label\n",
    "phrase = 'I loved this movie so much. Much more than I expeected, would recommend to anyone'\n",
    "\n",
    "# Recupera label\n",
    "label = lstm_model.predict(\n",
    "    phrase=phrase,\n",
    "    bigrams=bigrams, \n",
    "    trigrams=trigrams\n",
    ")\n",
    "\n",
    "# Apresenta classificação:\n",
    "print(f'Label (0=neg, 1=pos) = {label}')\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Label (0=neg, 1=pos) = 1\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('neural-nets': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "interpreter": {
   "hash": "78cf2962d9ecd331e0c03a399c9bb196d0486e280419aeeca2938a046faa0426"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}